{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W2L3.Logistic Regression Cost Function.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPOAjkQivWW/CbgrAcMGIrD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Logistic Regression Cost Function"],"metadata":{"id":"VXRxCpOOHXcl"}},{"cell_type":"markdown","source":["## **1.Loss function이란**\n","---\n","* 출력값과 사용자가 원하는 출력값의 오차를 의미\n","* 훈련 샘플 하나에 관하여 정의돼서 그 하나가 얼마나 잘 예측 되었는지 측정해줍니다"],"metadata":{"id":"u7m2vyL5V7xv"}},{"cell_type":"markdown","source":["### **Loss function식** \n","알고리즘이 출력한 y의 예측값과 참 값 y의\n","\n","제곱 오차의 반으로 손실 함수를 정의할 수 있습니다\n","\n","$$L(\\hat{y},y) = \\frac{1}{2}(\\hat{y}- y)^{2}$$ \n","\n","하지만 Logistic Regression 에서는 주로 사용하지 않습니다\n"," * 파라미터들을 학습하기 위해 풀어야할 최적화 함수가 Convex하지 않기 때문입니다\n","그러므로 여러 개의 지역 최적값을 가지고 있게 되어 문제가 생깁니다"],"metadata":{"id":"bmF5kV1BVrnJ"}},{"cell_type":"markdown","source":["Logistic Regression에서는 밑의 Loss function을 대신 사용합니다."],"metadata":{"id":"4P48aiPVXLGZ"}},{"cell_type":"markdown","source":["\n","$$L(\\hat{y},y) = -(ylog\\hat{y} + (1-y)log(1- \\hat{y}))$$\n"],"metadata":{"id":"fdsLcaY3W6mI"}},{"cell_type":"markdown","source":["## **2. 제곱오차반을 쓰지 않는 이유**\n","---"],"metadata":{"id":"_VTol1k_ZjVI"}},{"cell_type":"markdown","source":["* IF $y=1$\n","\n","Loss function은 그냥 −log(y의 예측값)이 됩니다\n","\n","$$ L(\\hat{y},y)=-log\\hat{y}$$\n","\n","$y=1$이기 때문에\n","$−log\\hat{y}$가 최대한 커지기를 원할 것입니다\n","\n","따라서 $\\hat{y}$ 이 최대한 커야 합니다\n","\n","하지만 $\\hat{y}$ 은 Sigmoid function 값이기 때문에 1보다 클 수 없습니다\n","\n","즉 $\\hat{y}$이 1보다 클 순 없으므로 1에 수렴하길 원한다는 뜻입니다\n","\n","\n"],"metadata":{"id":"bM0CnwjiHuNX"}},{"cell_type":"markdown","source":["* IF $y=0$\n"," \n","Loss function는 −log(1−y의 예측값)이 됩니다\n","$$ L(\\hat{y},y)=-log(1- \\hat{y})$$\n","\n","Loss function 값을 줄이고 싶다면\n","\n","$log(1−\\hat{y})$이 최대한 커야 합니다\n","\n","즉 $\\hat{y}$이 최대한 작아야 한다는 것을 알 수 있습니다\n","\n","$\\hat{y}$은 0과 1사이어야 하므로 $y$가 0이면 Loss function는\n","\n","$\\hat{y}$이 0에 수렴하도록 파라미터들을 조정할 것입니다\n"],"metadata":{"id":"7lyVUVEUZbno"}},{"cell_type":"markdown","source":[" *  $y$가 1일 때 $\\hat{y}$이 크고 y가 0일 때 $\\hat{y}$이 작은성질을 가지고 있는 함수들은 많기때문에 제곱오차를 쓰지 않는다.\n"],"metadata":{"id":"oZeSK7eoaEwg"}},{"cell_type":"markdown","source":["## **3. Cost Function 이란**\n","---\n","\n","훈련 세트 전체에 대해 Logistic regression 모델이 얼마나 잘 추측되었는지 측정해주는 함수이다.\n","\n"],"metadata":{"id":"mmOHuMHmazGx"}},{"cell_type":"markdown","source":["###**Cost Function 식**\n","$$J(w,b) =\\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y^{i}},y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m}[ylog\\hat{y} + (1-y)log(1- \\hat{y})]$$\n","\n","* Cost Function J는 파라미터 w와 b에 대해 Loss function를 각각의 훈련 샘플에 적용한 값의 합들의 평균 즉 m으로 나눈 값이다."],"metadata":{"id":"Z25EsbrEb-HY"}},{"cell_type":"markdown","source":["## **4.결론** \n","---\n"],"metadata":{"id":"FY_RSdK3H1hn"}},{"cell_type":"markdown","source":["* Logistic regression 모델을 학습하는 것이란 Cost Function 를 최소화해주는 파라미터들 w와 b를 찾는 것이다."],"metadata":{"id":"QD31sEjqcK6x"}},{"cell_type":"markdown","source":["### 참조\n","https://www.youtube.com/watch?v=SHEPb1JHw5o&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=9"],"metadata":{"id":"2jb52g_781oR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLvyR40mGm3b"},"outputs":[],"source":[]}]}